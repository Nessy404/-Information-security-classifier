<doc><category auto="true" type="str" verify="true"><![CDATA[Информационная безопасность]]></category><author auto="true" type="str" verify="true"><![CDATA[Kaspersky_Lab]]></author><title auto="true" type="str" verify="true"><![CDATA[Security Week 11: RSA 2019 и светлое будущее]]></title><keywords auto="true" type="list" verify="true"><item type="str"><![CDATA[Информационная безопасность]]></item><item type="str"><![CDATA[Блог компании «Лаборатория Касперского»]]></item></keywords><text auto="true" type="str" verify="true"><![CDATA[На прошлой неделе состоялась двадцать восьмая по счету конференция RSA, и если в 2018 году это крупнейшее в отрасли бизнес-мероприятие отметилось некоторыми затруднениями в поиске новых смыслов, в этот раз опять все хорошо. Открывающая презентация президента RSA Рохита Гая была посвящена «ландшафту доверия», и в ходе нее была сделана попытка нарисовать позитивный сценарий будущего, конкретно — 2049 года. Позитивный он потому, что удалось решить множество проблем сегодняшнего дня, в том числе даже не вопросы кибербезопасности, а, скорее, сложности с новой моделью развития общества, практически полностью завязанного на Интернет и цифровые сервисы. Действительно, если попробовать посмотреть на эту тему сверху (чем традиционно любят заниматься на RSA), дело уже не только в возможности взломать чей-то компьютер или сервер. Есть, например, проблема манипуляции в социальных сетях, да и сами сервисы подчас развиваются куда-то не туда. В таком контексте идея доверия — пользователей к компаниям, людей к искусственному интеллекту — действительно важная. Если вам интересно, как говорят о безопасности на бизнес-мероприятиях, смотрите видео. В презентации была высказана еще одна интересная мысль: искусственный интеллект не нужно заставлять выполнять задачи, которые под силу только людям, где факты играют меньшую роль по сравнению с эмоциями и, например, вопросами этики. И наоборот: решения, в которых требуется четкое следование фактам, стоит чаще отдавать на откуп машинам, которые (предположительно) менее склонны к ошибкам. Решения о том, доверять ли какому-то источнику информации в сети или нет, должны основываться на репутации. Похожая идея может быть применима и к проблеме киберинцидентов: да, они рано или поздно происходят у всех, но преимущество имеют организации, у которых объем усилий по защите данных клиентов перевешивает последствия взлома. Теоретические атаки на алгоритмы машинного обучения Впрочем, RSA по-прежнему показывает, насколько сложны взаимоотношения внутри индустрии между теми, кто находит новые проблемы, и теми, кто предлагает решения. Не считая пары протокольных речей, наиболее интересные выступления на конференции если и рисуют будущее, то в каких-то не очень оптимистичных тонах. Заслуживает внимания презентация представителя компании Google Николаса Карлини ( новость ). Он обобщил имеющийся опыт атак на алгоритмы машинного обучения, начав с этого уже классического примера из 2017 года: Оригинальное изображение кошки модифицируется совершенно незаметно для человека, но алгоритм распознавания классифицирует эту картинку совсем по-другому. Какую угрозу несет такая модификация? Еще один не самый свежий, но познавательный пример: Дорожный знак вроде бы немного пострадал от вандалов, но для человека он вполне понятен. Машина же может распознать знак с такими изменениями как совсем другой знак — с информацией об ограничении скорости, и останавливаться на перекрестке не станет. Дальше еще интереснее: Такой же метод можно применить и к звуку, что и было продемонстрировано на практике. В первом примере система распознавания речи «распознала» текст в музыкальном фрагменте. Во втором незаметная для человека манипуляция записи голоса привела к распознаванию совершенно другого набора слов (как на картинке). В третьем случае текст был распознан и вовсе из бессмысленного шума. Это интересная ситуация: в какой-то момент люди и их цифровые помощники начинают видеть и слышать совершенно разные вещи. Наконец, алгоритмы машинного обучения теоретически могут раскрывать персональные данные, на которых они были натренированы, при дальнейшем использовании. Самый простой и понятный пример — системы предиктивного набора текста ака «проклятый Т9». Безопасность медицинских устройств Безопасность в медицине в последнее время обсуждается по направлениям невероятно устаревшего софта и отсутствия бюджетов на развитие IT. Как результат — последствия кибератак серьезнее, чем обычно, причем речь идет о потере или утечке весьма чувствительных данных пациентов. На конференции RSA специалисты компании Check Point Software поделились результатами исследования компьютерной сети реального госпиталя в Израиле. В большинстве медицинских заведений компьютерная сеть не разделена на зоны, поэтому достаточно легко было обнаружить специализированные устройства, в данном случае — прибор для ультразвукового исследования. Рассказ о поиске уязвимостей в компьютерной части устройства был очень коротким. УЗИ работает под управлением Windows 2000, и найти эксплойт для одной из критических уязвимостей в этой ОС не составило труда. Исследователи получили доступ к архиву снимков с именами пациентов, смогли редактировать эту информацию и имели возможность активировать троян-вымогатель. Производитель устройства сообщил, что более современные модели построены на современном же софте, там регулярно доставляются обновления ПО (однако не факт, что устанавливаются), но обновление медицинских приборов стоит (много) денег, и какой в этом смысл, если и старые устройства работают? Рекомендации для медицинских организаций понятные: сегментация локальной сети, отделение устройств, хранящих приватные данные, от всех остальных. Интересно, что для развития технологий машинного обучения в медицине нужен, наоборот, максимально широкий доступ к данным пациентов — для тренировки алгоритмов. Disclaimer: Мнения, изложенные в этом дайджесте, могут не всегда совпадать с официальной позицией «Лаборатории Касперского». Дорогая редакция вообще рекомендует относиться к любым мнениям со здоровым скептицизмом.]]></text></doc>